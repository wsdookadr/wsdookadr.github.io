<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>scraping on Blog</title><link>https://wsdookadr.github.io/tags/scraping/</link><description>Recent content in scraping on Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 22 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://wsdookadr.github.io/tags/scraping/index.xml" rel="self" type="application/rss+xml"/><item><title>Building offline archives</title><link>https://wsdookadr.github.io/posts/p8/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://wsdookadr.github.io/posts/p8/</guid><description>Intro I’ve been looking into some ways to work offline.
Here are some reasons for that:
Decrease in the quality of general purpose search engine results
More targetted searches
Better response times and decreased latency for slow websites (since after I download them they’re served from my local network, maybe directly from the disk of my laptop)
Sites are disappearing at a high rate.</description></item><item><title>Polygon gridding using Geopandas and Shapely</title><link>https://wsdookadr.github.io/posts/p4/</link><pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate><guid>https://wsdookadr.github.io/posts/p4/</guid><description>Intro This post will discuss some work involving maps I’ve helped a client with. The main goal of the project was collecting various datasets from web services.
One of those web services has an endpoint that receives as a parameter a series of points that define a polygon for which the API request is made (the response will be a series of resources that are located inside that polygon). The API supports pagination, so if the area of the polygon is too big, we’ll have to do additional requests for all the result pages.</description></item></channel></rss>