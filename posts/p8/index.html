<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Building offline archives &#183; Blog</title><link rel=prev href=/posts/p7/><link rel=next href=/posts/p9/><meta name=description content="Intro I’ve been looking into some ways to work offline.
 Here are some reasons for that:
   Decrease in the quality of general purpose search engine results
  More targetted searches
  Better response times and decreased latency for slow websites (since after I download them they’re served from my local network, maybe directly from the disk of my laptop)
  Sites are disappearing at a high rate."><meta name=keywords content="offline,web-archives,crawlers,browsers,mitmproxy,scraping"><meta property="og:title" content="Building offline archives"><meta property="og:description" content="Intro I’ve been looking into some ways to work offline.
 Here are some reasons for that:
   Decrease in the quality of general purpose search engine results
  More targetted searches
  Better response times and decreased latency for slow websites (since after I download them they’re served from my local network, maybe directly from the disk of my laptop)
  Sites are disappearing at a high rate."><meta property="article:published_time" content="2022-08-22T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-22T00:00:00+00:00"><meta property="og:site_name" content="Blog"><meta property="article:section" content="posts"><meta property="article:tag" content="offline"><meta property="article:tag" content="web-archives"><meta property="article:tag" content="crawlers"><meta property="article:tag" content="browsers"><meta property="article:tag" content="mitmproxy"><meta property="article:tag" content="scraping"><meta name=twitter:card content="summary"><meta name=twitter:title content="Building offline archives"><meta name=twitter:description content="Intro I’ve been looking into some ways to work offline.
 Here are some reasons for that:
   Decrease in the quality of general purpose search engine results
  More targetted searches
  Better response …"><meta name=generator content="Hugo 0.79.1"><link rel=stylesheet href=/css/site.min.72989e8586a2239ea67ac4abbdb14c93dda32429d05c83e1eb621be0d97d1924.css integrity="sha256-cpiehYaiI56mesSrvbFMk92jJCnQXIPh62Ib4Nl9GSQ="></head><body class=article><header class=header><nav class=navbar><div class=navbar-brand><a class=navbar-item href=/>Blog</a>
<button class=navbar-burger data-target=topbar-nav>
<span></span><span></span><span></span></button></div><div id=topbar-nav class=navbar-menu><div class=navbar-end><a class=navbar-item href=/services/ title=Services>Services</a>
<a class=navbar-item href=/supported-tech/ title="Supported Technologies">Supported Technologies</a></div></div></nav></header><div class=body><div class=nav-container><aside class=nav><div class=panels><div class="nav-panel-menu is-active" data-panel=menu><nav class=nav-menu><h3 class=title><a href=/>Blog</a></h3><ul class=nav-list><li class=nav-item data-depth=0><button class=nav-item-toggle></button>
<a class=nav-link href=/posts/>Posts</a><ul class=nav-list><li class=nav-item data-depth=1><a class=nav-link href=/posts/p9/>Solving a simple puzzle using SymPy</a></li><li class="nav-item is-current-page" data-depth=1><a class=nav-link href=/posts/p8/>Building offline archives</a></li><li class=nav-item data-depth=1><a class=nav-link href=/posts/p7/>Packing, encrypting and uploading deliverables</a></li><li class=nav-item data-depth=1><a class=nav-link href=/posts/p6/>Multiple instance Activitywatch remote server setup for time tracking</a></li><li class=nav-item data-depth=1><a class=nav-link href=/posts/p5/>Creating mosaics, clipping and removing overlapping satellite images</a></li><li class=nav-item data-depth=1><a class=nav-link href=/posts/p4/>Polygon gridding using Geopandas and Shapely</a></li><li class=nav-item data-depth=1><a class=nav-link href=/posts/p3/>Fast sub-tree containment checks</a></li><li class=nav-item data-depth=1><a class=nav-link href=/posts/p2/>Non-standard solutions to some technical problems</a></li><li class=nav-item data-depth=1><a class=nav-link href=/posts/p1/>Setting up the new blog</a></li></ul></li><li class=nav-item data-depth=0><a class=nav-link href=/services/>Services</a></li><li class=nav-item data-depth=0><a class=nav-link href=/supported-tech/>Supported Technologies</a></li></ul></nav></div><div class=nav-panel-explore data-panel=explore style=display:none><div class=context><span class=title></span><span class=version></span></div></div></div></aside></div><main class=article><div class=toolbar role=navigation><button class=nav-toggle></button>
<a href=/ class=home-link></a><nav class=breadcrumbs aria-label=breadcrumbs><ul><li><a href=/>Blog</a></li><li><a href=/posts/>Posts</a></li><li>Building offline archives</li></ul></nav></div><div class=content><article class=doc><h1 class=page>Building offline archives</h1><div class="openblock is-before-toc"><div class=title>Posted on August 22, 2022
&#183; Tagged with
<a href=/tags/offline/>offline</a>, <a href=/tags/web-archives/>web-archives</a>, <a href=/tags/crawlers/>crawlers</a>, <a href=/tags/browsers/>browsers</a>, <a href=/tags/mitmproxy/>mitmproxy</a>, <a href=/tags/scraping/>scraping</a></div></div><div class=sect1><h2 id=_intro>Intro</h2><div class=sectionbody><div class=paragraph><p>I’ve been looking into some ways to work offline.</p></div><div class=paragraph><p>Here are some reasons for that:</p></div><div class=ulist><ul><li><p>Decrease in the quality of <a href="https://news.ycombinator.com/item?id=32201420">general purpose search engine results</a></p></li><li><p>More targetted searches</p></li><li><p>Better response times and decreased latency for slow websites (since after I download them they’re served from my local network, maybe directly from the disk of my laptop)</p></li><li><p>Sites are disappearing at a <a href=https://ccampbell.io/posts/10-percent-of-top-million-sites-are-dead/>high rate</a>.
Some content you bookmark today might be gone tommorow.</p></li><li><p>Recently read this post that makes a lot of predictions about the future. One of them says this: <a href=https://matduggan.com/programming-in-the/><em>"working asynchronously is going to be the new norm"</em></a>.
The post goes in more detail about what that means.</p></li></ul></div></div></div><div class=sect1><h2 id=_main_pipeline>Main pipeline</h2><div class=sectionbody><div class=imageblock><div class=content><svg viewBox="0 0 657 111" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" width="873" height="147"><g class="graph" id="graph0" transform="scale(1 1) rotate(0) translate(4 107)"><title>G</title><polygon fill="#fff" points="-4,4 -4,-107 653,-107 653,4 -4,4" stroke="transparent"/><g class="node" id="node1"><title>p0</title><polygon fill="none" points="81,-59 0,-59 0,-21 81,-21 81,-59" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="40.5" y="-43.8">generate</text><text font-family="Times,serif" font-size="14" text-anchor="middle" x="40.5" y="-28.8">sitemaps</text></g><g class="node" id="node2"><title>p1</title><polygon fill="none" points="188,-58 117,-58 117,-22 188,-22 188,-58" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="152.5" y="-36.3">crawler</text></g><g class="edge" id="edge1"><title>p0->p1</title><path d="M81.21-40C89.53-40 98.34-40 106.82-40" fill="none" stroke="#000"/><polygon fill="#000" points="106.94,-43.5 116.94,-40 106.94,-36.5 106.94,-43.5" stroke="#000"/></g><g class="node" id="node3"><title>p2</title><polygon fill="none" points="296,-81 235,-81 235,-43 296,-43 296,-81" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="265.5" y="-65.8">WARC</text><text font-family="Times,serif" font-size="14" text-anchor="middle" x="265.5" y="-50.8">files</text></g><g class="edge" id="edge2"><title>p1->p2</title><path d="M188.01-46.84C199.75-49.17 212.95-51.78 225.04-54.18" fill="none" stroke="#000"/><polygon fill="#000" points="224.39,-57.62 234.88,-56.13 225.75,-50.75 224.39,-57.62" stroke="#000"/></g><g class="node" id="node7"><title>p6</title><polygon fill="none" points="307,-36 224,-36 224,0 307,0 307,-36" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="265.5" y="-14.3">HAR files</text></g><g class="edge" id="edge6"><title>p1->p6</title><path d="M188.01-33.16C196.15-31.55 205-29.79 213.7-28.07" fill="none" stroke="#000"/><polygon fill="#000" points="214.65,-31.45 223.78,-26.07 213.29,-24.58 214.65,-31.45" stroke="#000"/></g><g class="node" id="node4"><title>p3</title><polygon fill="none" points="485,-103 343,-103 343,-65 485,-65 485,-103" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="414" y="-87.8">indexing for</text><text font-family="Times,serif" font-size="14" text-anchor="middle" x="414" y="-72.8">targetted queries</text></g><g class="edge" id="edge3"><title>p2->p3</title><path d="M296.15-66.45C306.94-68.07 319.72-69.99 332.8-71.96" fill="none" stroke="#000"/><polygon fill="#000" points="332.42,-75.44 342.83,-73.46 333.46,-68.52 332.42,-75.44" stroke="#000"/></g><g class="node" id="node5"><title>p4</title><polygon fill="none" points="457,-58 371,-58 371,-22 457,-22 457,-58" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="414" y="-36.3">warc2zim</text></g><g class="edge" id="edge4"><title>p2->p4</title><path d="M296.15-57.55C314.7-54.76 339.11-51.1 360.83-47.84" fill="none" stroke="#000"/><polygon fill="#000" points="361.54,-51.27 370.91,-46.32 360.5,-44.35 361.54,-51.27" stroke="#000"/></g><g class="node" id="node6"><title>p5</title><polygon fill="none" points="649,-66.5 521,-66.5 521,-13.5 649,-13.5 649,-66.5" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="585" y="-51.3">ZIM file</text><text font-family="Times,serif" font-size="14" text-anchor="middle" x="585" y="-36.3">can be browsed</text><text font-family="Times,serif" font-size="14" text-anchor="middle" x="585" y="-21.3">offline</text></g><g class="edge" id="edge5"><title>p4->p5</title><path d="M457.21-40C473.46-40 492.52-40 510.86-40" fill="none" stroke="#000"/><polygon fill="#000" points="510.95,-43.5 520.95,-40 510.95,-36.5 510.95,-43.5" stroke="#000"/></g></g></svg></div></div><div class=paragraph><p>This is the main pipeline. At the end of it, a ZIM file is produced which can be viewed offline by way
of <a href=https://download.kiwix.org/release/kiwix-tools/>kiwix-tools</a>.</p></div><div class=paragraph><p>Running the following will start a web server that’s able to serve all the pages for offline viewing.</p></div><div class=listingblock><div class=content><pre class="pygments highlight"><code data-lang=bash><span></span>kiwix-serve -i <span class=tok-m>127</span>.0.0.1 --threads <span class=tok-m>30</span> --port <span class=tok-m>8083</span> big.zim</code></pre></div></div><div class=paragraph><p>There’s also a new archive format called
<a href=https://github.com/webrecorder/specs>WACZ</a>, and
<a href=https://pywb.readthedocs.io/en/latest/index.html>pywb</a> is able to
replay from both WARC and WACZ.</p></div><div class=paragraph><p>Both pywb and kiwix-serve, rely on <a href=https://github.com/webrecorder/wombat>wombat.js</a> on the client-side which
makes use of <a href=https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API>service workers</a> to rewrite
all requests made by a page and have them point to the local archive instead of the original live urls.</p></div></div></div><div class=sect1><h2 id=_crawlers_for_dynamic_web_pages>Crawlers for dynamic web pages</h2><div class=sectionbody><div class=sect2><h3 id=_splash>Splash</h3><div class=paragraph><p>This crawler predates puppeteer by about 4 years. It’s part of the larger
Scrapy eco-system which is very popular for writing crawlers. Actually
it’s this component that allows Scrapy to crawl dynamic web pages.</p></div><div class=paragraph><p>Splash uses Webkit bindings, spawns the browser engine provided by
<code>PyQt5.QtWebKit</code> inside Python, it then wraps various callbacks or
overrides them with its own and uses Twisted to attach to those callbacks.
The first time I saw this I thought it was ingenious, I still think it is.
Scrapy is also based on Twisted and makes extensive use of event-based
programming.</p></div><div class=paragraph><p>I found the following sources informative about Splash:
<a href="https://old.reddit.com/r/Python/comments/2xp5mr/handling_javascript_in_scrapy_with_splash/cp2vgd6/?context=3">source1</a>,
<a href=https://github.com/scrapinghub/splash/blob/ab28b0233c245461189881f1f5656b96371a4b40/splash/engines/webkit/browser_tab.py#L62>source2</a>,
<a href=https://github.com/scrapinghub/splash/blob/master/docs/internals/js-python-lua.rst>source3</a></p></div><div class=paragraph><p>Splash also embeds a Lua interpreter and sandbox it in the browser
engine, which allows you to send Lua code from Python that gets executed
in the browser engine and sends the result back to Python.</p></div><div class=paragraph><p>Even though their latest docker image is from 2020, now it can still
produce HAR files, but the code hasn’t seen much updates lately,
and a series of issues have started to crop up:</p></div><div class=ulist><ul><li><p>the browser engine or browser used in the docker image is too old <a href=https://github.com/scrapinghub/splash/issues/1147>1147</a>, <a href=https://github.com/scrapinghub/splash/issues/1152>1152</a>, <a href=https://github.com/scrapinghub/splash/issues/1122>1122</a>, <a href=https://github.com/scrapinghub/splash/issues/1094>1094</a></p></li><li><p>incompatible with modern js frameworks <a href=https://github.com/scrapinghub/splash/issues/1117>1117</a></p></li><li><p>memory leaks <a href=https://github.com/scrapinghub/splash/issues/1042>1042</a>, <a href=https://github.com/scrapinghub/splash/issues/1049>1049</a>, <a href=https://github.com/scrapinghub/splash/issues/757>757</a></p></li><li><p>some dependency problems are preventing a new build <a href=https://github.com/scrapinghub/splash/issues/1136>1136</a></p></li></ul></div><div class=paragraph><p>Splash’s approach is actually much closer to the events that happen
in the browser but it signs up for the burden of playing catch-up with
browser updates/changes.</p></div></div><div class=sect2><h3 id=_browsertrix_crawler>Browsertrix-crawler</h3><div class=paragraph><p>This project has quite some momentum behind it because it’s used by at least 3 different
orgs on Github:
<a href=https://github.com/openzim>openzim</a>, <a href=https://github.com/kiwix>kiwix</a> and <a href=https://github.com/webrecorder>webrecorder</a>.</p></div><div class=paragraph><p>The main node.js script is <a href=https://github.com/webrecorder/browsertrix-crawler/blob/e22d95e2f07ef8a4cd3a4c309ee9ca0d6bab559e/crawler.js>crawler.js</a> which starts a <a href=https://github.com/thomasdondorf/puppeteer-cluster/blob/fa1ccbe49912c9b49182da5338247471e2d912c4/src/Cluster.ts#L86>puppeteer Cluster</a>
and an in-memory job queue, and an uwsgi server running pywb (see
<a href=https://github.com/webrecorder/browsertrix-crawler/blob/e22d95e2f07ef8a4cd3a4c309ee9ca0d6bab559e/config/uwsgi.ini>config/uwsgi.ini</a> ).</p></div><div class=paragraph><p>There is a job queue of the pages that were crawled ( see
<a href=https://github.com/webrecorder/browsertrix-crawler/blob/e22d95e2f07ef8a4cd3a4c309ee9ca0d6bab559e/util/state.js>util/state.js</a> ).
The purpose of this queue is the same as in most crawlers, it keeps track
of all urls seen, but also those in progress and those that are pending.</p></div><div class=paragraph><p>The pywb instance has both a recorder and a proxy mode (see
<a href=https://github.com/webrecorder/pywb/blob/f0340c6898626310a41bb87bfba8fa15df5ee87b/pywb/apps/frontendapp.py>pywb/apps/frontendapp.py</a>).
From what I can tell it runs in both modes.
The proxy mode should be responsible for mediating the data flow between
the browser and the web pages it wants to access whereas the recorder
mode should be responsible for generating the WARC records and writing
them to disk.</p></div><div class=paragraph><p>The recorder server lives here <a href=https://github.com/webrecorder/pywb/blob/f0340c6898626310a41bb87bfba8fa15df5ee87b/pywb/recorder/recorderapp.py>pywb/recorder/recorderapp.py</a> and it’s
called RecorderApp.</p></div><div class=paragraph><p>The pywb instance will act as a proxy server that mediates all transfers
between crawler.js and the browser. It will also store and serialize
all HTTP requests as WARC records.</p></div><div class=paragraph><p>Let’s imagine a scenario where a single static page is crawled.
Here’s a rundown of this scenario:</p></div><div class="olist arabic"><ol class=arabic><li><p>crawler.js starts (in turn it will start a puppeteer cluster)</p></li><li><p>the url is enqueued onto the in-memory job queue (or redis)</p></li><li><p>puppeteer-cluster picks up the task from the job queue and checks if
it was already crawled, otherwise it passes it to one of its workers</p></li><li><p>once it reaches a worker(a pupeteer instance) it gets passed to a browser instance.</p></li><li><p>this browser instance is actually set up to use a proxy-server. in fact
this proxy server is the pywb instance mentioned above.</p></li><li><p>pywb mediates this HTTP request and already builds a WARC record for the HTTP request
it has received</p></li><li><p>like all proxies, pywb performs this HTTP request (on behalf of the browser which
originally made the request).</p></li><li><p>the HTTP request finishes and an HTTP response is sent back to pywb.</p></li><li><p>pywb receives the response, builds a WARC record for it, but also
sends back the HTTP response to the browser.
at this point pywb might also write the WARC record to disk</p></li></ol></div><div class=paragraph><p>Now here’s a count for the number of event loops that are present in this scenario:</p></div><div class=ulist><ul><li><p>uwsgi</p></li><li><p>pywb and gevent</p></li><li><p>crawler.js</p></li><li><p>one for each puppeteer instance launched by puppeteer-cluster</p></li></ul></div><div class=paragraph><p>Even in this scenario there might be things I’ve missed. But even if I
capture all the details for that scenario, there are a lot more features
I haven’t covered pywb. I could probably fill 4 more blog posts just to
describe how all of these work.</p></div><div class=paragraph><p><code>pywb</code> also has functionality to play back WARC files directly (I haven’t tried
out that part yet)</p></div><div class=paragraph><p>At the time of writing this, browsertrix-crawler has the following open issues:</p></div><div class=ulist><ul><li><p>timeout logic require a bit more work <a href=https://github.com/webrecorder/browsertrix-cloud/issues/298>298</a>, <a href=https://github.com/webrecorder/browsertrix-crawler/issues/146>146</a></p></li><li><p>high CPU usage <a href=https://github.com/webrecorder/browsertrix-crawler/issues/106>106</a></p></li></ul></div><div class=paragraph><p>UPDATE: Already seeing commits that address some of these open issues.</p></div></div><div class=sect2><h3 id=_femtocrawl>Femtocrawl</h3><div class=paragraph><p>I’ve done some code-reading of multiple solutions, was able to look at
pros and cons, realized that I really need just parts of these solutions
and decided to write a simplified one that works for me. I wrote it
because it’s smaller, has fewer components and it’s easier for me to
reason about and to debug it. Of course it doesn’t have all the
functionalities of the other solutions (browser behaviors and screencasting
to name a few).</p></div><div class=paragraph><p>I’ve uploaded it <a href=https://github.com/wsdookadr/femtocrawl/>on Github here</a>.</p></div><div class=paragraph><p>To be able to estimate when a crawl would end, I’ve noticed it’s easier
in my case to build the sitemap as a separate step instead of letting
the crawler decide which urls to recurse into. So I’d rather build that
sitemap as a separate process before I start crawling. This makes the
entire process more predictable.</p></div><div class=paragraph><p>Some websites have a <code>/sitemap.xml</code> endpoint, some don’t. Some will have
it under a different name and listed in <code>/robots.txt</code>. Then some of them
will have a nested <code>/sitemap.xml</code> which contains links to more specific
sitemaps. And then for some the sitemap will be completely missing and a complete
crawl will be required to find all the urls.</p></div><div class=paragraph><p>Doing this complete crawl could actually be avoided if <a href=https://commoncrawl.org/>Common Crawl</a>
is queried instead, but that won’t have the latest data, actually I’m not sure how often CC runs.
But even using a stale pre-computed CC sitemap should improve the speed of a crawl, even if
you’d have to check status codes for all urls and figure out if there’s any new urls you haven’t
seen, there’s still less to process than if you were to do it from scratch.</p></div><div class=paragraph><p>For some pages the crawler might not have enough time to fetch all the
resources, but that’s okay because a second pass is possible to either:</p></div><div class=ulist><ul><li><p>pick up all the failed resources from the HAR files and fetch them</p></li><li><p>parse out the WARC records with text/html mimetype, find out which of them
are absent from the WARC and fetch them</p></li></ul></div><div class=paragraph><p>These two methods above are not equivalent, but if used together they should
bring in most of the content. There’s just one exception which are expiring resources
which are not handled, but I’m okay with those not being fetched.</p></div><div class=paragraph><p>Another part I’ve noticed is the WARC files produced might not be compatible with <code>warc2zim</code>.
Some web pages have really odd HTTP responses (corner-cases of the HTTP spec).
And usually if I encounter such a corner-case(here’s <a href=https://github.com/openzim/warc2zim/issues/94>an example</a>), I
can either report it as an issue or exclude it somehow.</p></div><div class=paragraph><p>The exclusion process is easy and fast before you join all WARC into one.
If you’ve already joined them, it will be complicated and error-prone
to eliminate a bad WARC record. Not only that but if you remove a WARC
record, an entire web page depends on it, together with all its resources,
so in fact, all WARC records belonging to that web page should be removed.</p></div><div class=paragraph><p>I’ve read through the WARC spec. One unanswered question for me is given
a WARC record, is it possible to know the web page it belongs to and all
WARC records that this web page depends on. I found the WARC spec somewhat
terse, maybe I’ll find some more examples about how all their fields work.</p></div><hr><div class=paragraph><p>Here’s a short diagram that explains how femtocrawl works:</p></div><div class=imageblock><div class=content><svg viewBox="0 0 623 89" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg" width="828" height="118"><g class="graph" id="graph0" transform="scale(1 1) rotate(0) translate(4 85)"><title>G</title><polygon fill="#fff" points="-4,4 -4,-85 619,-85 619,4 -4,4" stroke="transparent"/><g class="node" id="node1"><title>p1</title><polygon fill="none" points="75,-59 0,-59 0,-23 75,-23 75,-59" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="37.5" y="-37.3">browser</text></g><g class="node" id="node2"><title>p2</title><polygon fill="none" points="206,-59 111,-59 111,-23 206,-23 206,-59" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="158.5" y="-37.3">mitmproxy</text></g><g class="edge" id="edge1"><title>p1->p2</title><path d="M75.17-41C83.22-41 91.93-41 100.57-41" fill="none" stroke="#000"/><polygon fill="#000" points="100.63,-44.5 110.63,-41 100.63,-37.5 100.63,-44.5" stroke="#000"/></g><g class="node" id="node3"><title>p3</title><polygon fill="none" points="339,-81 267,-81 267,-45 339,-45 339,-81" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="303" y="-59.3">website</text></g><g class="edge" id="edge2"><title>p2->p3</title><path d="M206.16-48.2C222.41-50.71 240.6-53.52 256.73-56.01" fill="none" stroke="#000"/><polygon fill="#000" points="256.58,-59.53 266.99,-57.6 257.64,-52.61 256.58,-59.53" stroke="#000"/></g><g class="node" id="node4"><title>p4</title><polygon fill="none" points="364,-38 242,-38 242,0 364,0 364,-38" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="303" y="-22.8">HAR output</text><text font-family="Times,serif" font-size="14" text-anchor="middle" x="303" y="-7.8">written to disk</text></g><g class="edge" id="edge3"><title>p2->p4</title><path d="M206.16-33.8C214.48-32.51 223.3-31.15 232.1-29.79" fill="none" stroke="#000"/><polygon fill="#000" points="232.64,-33.25 241.99,-28.26 231.57,-26.33 232.64,-33.25" stroke="#000"/></g><g class="node" id="node5"><title>p5</title><polygon fill="none" points="484,-37 400,-37 400,-1 484,-1 484,-37" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="442" y="-15.3">har2warc</text></g><g class="edge" id="edge4"><title>p4->p5</title><path d="M364.09-19C372.68-19 381.44-19 389.84-19" fill="none" stroke="#000"/><polygon fill="#000" points="389.85,-22.5 399.85,-19 389.85,-15.5 389.85,-22.5" stroke="#000"/></g><g class="node" id="node6"><title>p6</title><polygon fill="none" points="615,-37 520,-37 520,-1 615,-1 615,-37" stroke="#000"/><text font-family="Times,serif" font-size="14" text-anchor="middle" x="567.5" y="-15.3">WARC files</text></g><g class="edge" id="edge5"><title>p5->p6</title><path d="M484.12-19C492.21-19 500.82-19 509.32-19" fill="none" stroke="#000"/><polygon fill="#000" points="509.58,-22.5 519.58,-19 509.58,-15.5 509.58,-22.5" stroke="#000"/></g></g></svg></div></div><div class=paragraph><p>There’s no puppeteer involved, no devtools protocol is used. Just a browser, mitmproxy
and har2warc. One piece that’s not present in the diagram is a custom Firefox profile
that’s required for adblocking and proxy settings.</p></div><div class=paragraph><p>The main use-cases are:</p></div><div class=ulist><ul><li><p>building offline web archives</p></li><li><p>website testing</p></li><li><p>cross-testing different web archiving tools</p></li></ul></div><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content>If you liked this article or if you have feedback and would like
to discuss more about data collection pipelines and scraping feel free to
get in touch at <a href=mailto:stefan.petrea@gmail.com>stefan.petrea@gmail.com</a>.</td></tr></tbody></table></div></div></div></div><nav class=pagination><span class=prev><a href=/posts/p7/>Packing, encrypting and uploading deliverables</a></span>
<span class=next><a href=/posts/p9/>Solving a simple puzzle using SymPy</a></span></nav></article><aside class="toc sidebar" data-title data-levels=2><div class=toc-menu></div></aside></div></main></div><footer class=footer><p>This page was built with <a href=https://gohugo.io/>Hugo 0.79.1</a> using the <a href=https://github.com/basil/antora-default-ui-hugo-theme>Hugo port</a> of the <a href=https://gitlab.com/antora/antora-ui-default>Antora default UI</a>. The source code for this UI is licensed under the terms of the <a href=https://www.mozilla.org/en-US/MPL/2.0/>Mozilla Public License, Version 2.0</a> (MPL-2.0).</p></footer><div id=copy-to-clipboard style=display:none data-svg=/img/octicons-16.svg></div><script src=/js/site.min.da461a26436bc4ba84975879f2d7f90a593245b694bbea075400ef251e78b2e0.js integrity="sha256-2kYaJkNrxLqEl1h58tf5ClkyRbaUu+oHVADvJR54suA="></script><script src=/js/vendor/highlight.pack.79c21a0aa45cc5756c18c07f0cf37be9ad1e1b728f6f0e6b210c3913cfef592f.js integrity="sha256-ecIaCqRcxXVsGMB/DPN76a0eG3KPbw5rIQw5E8/vWS8="></script><script>;[].slice.call(document.querySelectorAll('pre code.hljs')).forEach(function(node){hljs.highlightBlock(node)})</script></body></html>