+++
title = "Non-standard solutions to some technical problems"
date = "2021-02-20"
markup = "asciidoc"
tags = ["databases","logs","monitoring","analytics","postgresql","interviews"]
+++

== Intro

When I'm interviewing for contracts or for jobs, many times I get asked
what are the hardest problems I've had to deal with. I'll write below
some of these problems as I remember them now.

== Double-counting

Some years ago, I was a consultant for a job in an analytics team. I
was tasked with finding a bug in code that was counting the views of a
top-10 Alexa website with hundreds of millions of views every month.

The code was counting the number of views coming from different
areas around the world. In order to summarize this vast amount
of data, and bucket it into geographical regions, it would have
to classify each city in the world to a certain region, and
there were two main regions that we were interested in:
link:https://en.wikipedia.org/wiki/Northern_Hemisphere[Northern Hemisphere]
vs. link:https://en.wikipedia.org/wiki/Southern_Hemisphere[Southern Hemisphere].

The bug itself would manifest in the totals row, for the percentages of views.
Even though the total should have always been `100%`, it would be `100.41%`.

It was definitely a case of link:https://en.wikipedia.org/wiki/Double_counting_(fallacy)[double-counting] but
nobody knew exactly where it was coming from.

The codebase was a fairly large 40k lines of code Perl written in a very
arcane pre-Perl5 codingstyle. Tests were absent and there was pushback
in trying to introduce them. Checking all arithmetic operations would
have taken way too much time and effort.

Making changes to the code and re-running it on full or partial data
was prohibitive because there were hundreds of gigabytes of data
to analyze.

After many days of looking at the code, and trying to understand where
the problem was coming from, I realized that the only chance I had was to try
to craft the simplest test possible.

Eventually I managed to craft that test: I wrote a program that would run the
Perl code with a one-line log file, each time with a different city on the planet.
Right after the Perl code was run, my program would check the reports generated
to see if the percentages were correct.

For a few islands in the Atlantic Ocean, it turned out that the
percentages were wrong.  Looking further I realized that this was
because the cities on those islands were mapped to both the Northern and the Southern hemispheres
(on this occasion I learned that there were link:https://en.wikipedia.org/wiki/Comma-separated_values[CSV] data
files that described this `City->Hemisphere` mapping). Then checking
the world map, it turned out some of those islands really were near
the equator.

== Dashboard dependencies

When I was working a devops position in an online gaming company, I was
handed a problem about dashboards being broken and reports in them not
showing up anymore.

The way these were structured was this: `Dashboards --> Reports --> Queries --> Tables`

So a dashboard had multiple reports, and reports were using queries,
and in turn queries were accessing data from tables.

The problem emerged after an upgrade of the visualization software involved.
Contacting the support team of the vendor resulted in an answer like: _"Just upgrade to the next version"_
(even when presented with an analysis of the problem).

I realized this wouldn't lead to a solution, wrote code to dig into
the metadata storage of the visualization software, reverse-engineered
the undocumented structure of the metadata, copied the data over to an
link:https://sqlite.org/index.html[SQLite] database (which I basically
used as a link:https://en.wikipedia.org/wiki/Graph_database[graph database] with nodes and edges) and came up with a
link:https://graphviz.org/[GraphViz]-based program that
was able to track object dependencies across upgrades. This
allowed me to then find the nodes that were pointed to by
link:https://en.wikipedia.org/wiki/Dangling_pointer[broken links] in the
dependency relations, and either re-create or update the objects in the
current version of the reports in order for them to work properly.

Having done all of this made it easy to solve the next
task, which was to identify unused tables, by computing the
link:https://en.wikipedia.org/wiki/Degree_(graph_theory)[in-degree]
of each such table.

== Identifying WAL size generated by queries

When I was working as a DBA at a telecom company we had some spikes in
the traffic of WAL logs in a PostgreSQL database that was replicated.

It turned out that I could actually use
link:https://www.postgresql.org/docs/11/pgwaldump.html[`pg_waldump`] to get
the transaction ids in each WAL file. In addition, I realized that if I
monitored the queries that were running on the database servers using the
link:https://www.postgresql.org/docs/11/monitoring-stats.html[`pg_stat_activity`]
catalog I could get the queries and their transaction ids. By joining
these two pieces of information, I was able to find out how much WAL
each query had generated, and then pick the outliers and report them
back to the teams that had written them, in order to be refactored.

== Conclusion

Sometimes seemingly hard bugs can be solved by crafting the right kind of
test. Sometimes they require crafting tools that increase observability
and visibility into the ways systems work.

[NOTE]
If you liked this article and would like to discuss more about how MAGNA SOFTWARE S.R.L
can help your business regarding link:https://wsdookadr.github.io/services/[production issues], feel
free to link:https://calendly.com/stefan-petrea/30min[book a 30-minute call] with me so we can get
to know each other and discuss a future collaboration.
 
